<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images">
  <meta name="keywords" content="URDFormer, real-to-sim, robot manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>URDFormer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4YJFFHEBP9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4YJFFHEBP9');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://qiuyuchen14.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://sites.google.com/view/implicitaugmentation/home">
            ISAGrasp
          </a>
          <a class="navbar-item" target="_blank" href="https://genaug.github.io/">
            URDFormer
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images</h1>
<!--          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">Under Submission</a></h3>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://qiuyuchen14.github.io/">Zoey Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://aaronwalsman.com/">Aaron Walsman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://memmelma.github.io/">Marius Memmel </a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://kaichun-mo.github.io/">Kaichun Mo</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=kD9uKC4AAAAJ&hl=en">Alex Fang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/karthikeya-vemuri/">Karthikeya Vemuri</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/alan-wu-501a93202/">Alan Wu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox*</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://abhishekunique.github.io/">Abhishek Gupta*</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Nvidia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2302.06671"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://urdformer.github.io/media/videos/GenAug_voice.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/urdformer/urdformer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <img src="https://urdformer.github.io/media/images/teaser_cab.gif" class="interpolation-image"
         alt="Interpolate start reference image." />
      <h2 class="subtitle has-text-centered">
      </br>
        We propose <span class="dcliport">URDFormer</span>, a pipeline towards large-scale simulation generation from real-world images.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
<!--        <h2 class="title is-3"><span class="dcliport">GenAug</span></h2>-->

        <!-- Interpolating. -->
        <h3 class="title is-4"> <b>URDFOrmer:</b>  Generative Augmentation for Real-World Data Collection</h3>
        <img src="https://urdformer.github.io/media/images/application.png" class="interpolation-image"
         alt="Interpolate start reference image." />
        <br/>
        <br/>
          Examples of application for URDFormer. (a) To safely deploy a robot in the real world, we first generate the "digital" version of the real-world scene, then train a robot in simulation and deploy in the real-world.
        (b) Since URDFormer is based on RGB images, we can potentially generate large-scale simulation environment from internet data that can emulate the real-world distrubtion. 
        <br/>

     </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision.
            Achieving this goal since it provides a realistic, targeted simulation playground for training generalizable decision-making systems.
            This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems.
            However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
            While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes,
            complete with ``natural" kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets.
            To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models.
            We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures
            from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments
            and an integrated system for training robust robotic control policies in the resulting environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
    <!-- Paper video. -->
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MxcmKKvdBhk?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">Real-World Experiments</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
       By training on a dataset extrapolated from only 10 demonstrations in this simple environment on the left,
      the robot is able
        </br>
        to solve the task in entirely different environments and objects.
    </h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/real_world1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/real_world1.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/real_world2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <br/>
      <br/>
    <h2 class="title is-3"><span class="dcliport">Simulation Experiments</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
       To further study in depth the effectiveness of GenAug, we conduct large-scale experiments with other baselines in simulation.
        </br>In particular, we organize baseline methods as (1) in-domain augmentation methods and (2) learning from out-of-domain priors.
      </h2>
      <br/>
      <br/>
      <h2 class="title is-3"><span class="dcliport">Table-top Manipulation Tasks</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
        </h2>
      <img src="https://urdformer.github.io/media/images/sim_table.png"  width="1200" height="600"/>
        <br/>
      <img src="https://urdformer.github.io/media/images/sim2.png"  width="800" height="600"/>
        <br/>
      <h2 class="title is-3"><span class="dcliport">Behavior Cloning Tasks</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
        </h2>
      </br>
      In addition, we show GenAug can apply to Behavior Cloning tasks such as ”close the top drawer”, with a different robot:"Fetch".
      <br/>In particular,  we collected 100 demonstrations and trained a CNN-MLP behavior cloning policy finetuned with R3M embeddings.
      <br/>The input is the RGB observation and the output is a 8-dim action vector.
      <br/>We tested on 100 unseen backgrounds using iGibson rooms and observed GenAug is able to achieve 60% success rate
      <br/>while policy without Genaug is only 1%, leading to almost 60% improvement.

      <br/>
      <img src="https://urdformer.github.io/media/images/genaug_bc_train.gif"  width="700" height="260" />
        <br/>
      <br/>

     <img src="https://urdformer.github.io/media/images/genaug_bc_test.gif"  width="780" height="320" />
      <br/>
  </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">Approach</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
        </br>
        1. <b>URDFormer</b>:
    </br>
        </br>
        Given a demonstration on one simple environment, GenAug can automatically add distractor objects,
        </br>
        change the object texture,  change object classes and change table and background.
    </h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/distractor.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/object.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/table.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://genaug.github.io/media/videos/texture.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero teaser">
   <div class="rows is-centered ">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
    </br>
        2. <b>Data Generation</b>
    </br>
      We randomly select the mode and augment the initial dataset into a large and diverse dataset.
      </h2>
      <video id="object" autoplay muted loop  height="100%" width="100%">
        <source src="https://urdformer.github.io/media/videos/augmented_data.mp4"
                type="video/mp4">
      </video>
    </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
    </br>
        3. <b>System</b>
    </br>
        We generate a large and diverse augmented dataset from a small amount of human demonstrations collected in a simple environment,
        and use this <b>augmented</b> dataset to train a language-conditioned robot policy and deploy it in the real-world.
      </h2>
      <img src="https://urdformer.github.io/media/images/training.png" class="interpolation-image"
         alt="Interpolate start reference image." />
        <br/>
    </div>
  </div>
</section>

<section class="hero teaser">
   <div class="rows is-centered ">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
    </br>
        4. <b>Data Collection</b>
    </br>
        To collect demonstrations in the real-world, a user generates a small dataset by specifying pick and place locations. These 2D locations are projected to the 3D points in the robot coordinates using calibrated depth maps.
      </h2>
      <video id="object" autoplay muted loop  height="100%" width="100%">
        <source src="https://urdformer.github.io/media/videos/datacollection.mp4"
                type="video/mp4">
      </video>
    </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-fullhd">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">BibTeX</span></h2>
    <pre><code>@article{chen2023genaug,
  title={GenAug: Retargeting behaviors to unseen situations via Generative Augmentation},
  author={Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  journal={arXiv preprint arXiv:2302.06671},
  year={2023}
}</code></pre>
  </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/cliport/clipoprt.github.io">CLIPort</a> made by the amazing <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and  <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
