<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images">
  <meta name="keywords" content="URDFormer, real-to-sim, robot manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>URDFormer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TX67N6B0Q7"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-TX67N6B0Q7');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://qiuyuchen14.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://sites.google.com/view/implicitaugmentation/home">
            ISAGrasp
          </a>
          <a class="navbar-item" target="_blank" href="https://genaug.github.io/">
            GenAug
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images</h1>
<!--          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">Under Submission</a></h3>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://qiuyuchen14.github.io/">Zoey Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://aaronwalsman.com/">Aaron Walsman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://memmelma.github.io/">Marius Memmel </a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://kaichun-mo.github.io/">Kaichun Mo</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=kD9uKC4AAAAJ&hl=en">Alex Fang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/karthikeya-vemuri/">Karthikeya Vemuri</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/alan-wu-501a93202/">Alan Wu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox*</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://abhishekunique.github.io/">Abhishek Gupta*</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Nvidia, </span>
            <span class="author-block"><sup>*</sup>Equal Advising</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://drive.google.com/file/d/1vXcBgf--ySQWeh3VFIiigAV8_cZi_Kzv/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://urdformer.github.io/media/videos/urdformer.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/WEIRDLabUW/urdformer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/teaser_final.gif">
            </figure>
            <p class="has-text-centered">
            We propose URDFormer, a scalable pipeline towards large-scale simulation generation from real-world images.
              Given an image (from the internet or captured from a phone), URDFormer predicts its corresponding interactive "digital twin"
              in the format of a URDF. This URDF can be loaded into a simulator to train a robot for different tasks.
            </p>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer Applications</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/URDFormer_application.gif" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
           (a) For the safe deployment of robots in real-world settings, we initially create a 'digital' replica of the real-world scene.
            Subsequently, we gather demonstrations in simulation and train a robot policy. This policy can then be safely deployed in the real world or used for further real-world fine-tuning.
            (b) Given that URDFormer relies solely on RGB images, it holds the potential to create extensive simulation environments from internet data, closely mirroring the real-world distribution.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision.
            Achieving this goal since it provides a realistic, targeted simulation playground for training generalizable decision-making systems.
            This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems.
            However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
            While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes,
            complete with ``natural" kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets.
            To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models.
            We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures
            from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments
            and an integrated system for training robust robotic control policies in the resulting environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
    <!-- Paper video. -->
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://urdformer.github.io/media/videos/urdformer.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

</section>

<!-- Line separator. -->
<div class="columns is-centered">
  <div class="column is-two-thirds">
    <hr class="is-divider" style="height: 4px;">
  </div>
</div>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Overview</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/pipeline.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
            To train a network that takes real-world images and predicts their corresponding URDFs, we propose a forward-inverse pipeline. During the forward phase,
            we first randomly sample a URDF (this could be from a dataset or procedurally generated) and load the corresponding articulated assets.
            However, if we train a network with the initial texture, it likely won't generalize well to real-world images due to the distribution shift.
            We leverage the power of image-conditioned stable diffusion (Robin Rombach et al., 2022) that was pretrained on a web-scale dataset and convert the initial synthetic images into photorealistic images.
            Now we automatically have paired realistic images with their corresponding URDFs to supervise a network. We can train the network on the generated images which emulate real-world complexity, such as lighting and textures,
            and enable the network to predict correct URDFs given real-world images during the inverse phase.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Line separator. -->
<div class="columns is-centered">
  <div class="column is-two-thirds">
    <hr class="is-divider" style="height: 4px;">
  </div>
</div>
<!--/ Line separator. -->


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Results</h2>
      <h2 class="title is-4">Convert Images into Simulation Content</h2>
      <h2 class="subtitle">URDFormer enables fast simulation content generation directly from images, whether they're sourced from the internet or captured with your phone.</h2>
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_cabinets.gif" alt="Predicted URDFs of cabinets with different configurations from internet images">
            </figure>
            <p class="has-text-centered">(a) Predicted URDFs for cabinets in various configurations directly from internet images.</p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_appliance.gif" alt="Predicted URDFs of different kitchen appliances from internet images">
            </figure>
            <p class="has-text-centered">(b) Predicted URDFs for kitchen appliances in various configurations directly from internet images.</p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_appliance.gif" alt="Predicted URDFs of different kitchen appliances from internet images">
            </figure>
            <p class="has-text-centered">(c) Predicted URDFs for kitchens with different layouts directly from internet images.</p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_phone_kitchens.gif" alt="Predicted URDFs of real-world kitchens captured by a phone">
            </figure>
            <p class="has-text-centered">(d) We also visualize several examples of predicted URDFs from real-world kitchens captured using a phone.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Application</h2>
      <h2 class="title is-4">1. Real-to-Sim-to-Real for safer real-world deployment</h2>
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/real_world1.gif">
            </figure>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/real_world2.gif">
            </figure>
             <p>Given a single image of an object, URDFormer predicts its URDF, automatically collects data,
               and trains a robot within a simulator. The learned policy can then be transferred to the real world for safer deployment.</p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section id="overview-videos">
      <div class="columns is-centered">
          <div class="column is-two-thirds">
            <h2 class="title is-3"> URDFormer enables faster real-to-sim-to-real without any manual effort </h2>
            <section class="hero is-light is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-chair-tp">
                      <video poster="" id="chair-tp" autoplay muted loop height="100%">
                        <source src="media/videos/robots/strecth.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-steve">
                      <video poster="" id="steve" autoplay muted loop height="100%">
                        <source src="media/videos/robots/open_top_left_drawer.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-fullbody">
                      <video poster="" id="fullbody" autoplay muted loop height="100%">
                        <source src="media/videos/robots/close_right_door.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-shiba">
                      <video poster="" id="shiba" autoplay muted loop height="100%">
                        <source src="media/videos/robots/open_door.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-blueshirt">
                      <video poster="" id="blueshirt" autoplay muted loop height="100%">
                        <source src="media/videos/robots/close_door.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-shiba">
                      <video poster="" id="shiba" autoplay muted loop height="100%">
                        <source src="media/videos/robots/fetch_object.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-shiba">
                      <video poster="" id="shiba" autoplay muted loop height="100%">
                        <source src="media/videos/robots/top_drawer.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                   <div class="item item-chair-tp">
                      <video poster="" id="chair-tp" autoplay muted loop height="100%">
                        <source src="media/videos/robots/collect_object.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-shiba">
                      <video poster="" id="shiba" autoplay muted loop height="100%">
                        <source src="media/videos/robots/top_middle_drawer.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    <div class="item item-shiba">
                      <video poster="" id="shiba" autoplay muted loop height="100%">
                        <source src="media/videos/robots/open_drawer.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                   <div class="item item-chair-tp">
                      <video poster="" id="chair-tp" autoplay muted loop height="100%">
                        <source src="media/videos/robots/close_drawer.mp4"
                                type="video/mp4">
                      </video>
                    </div>
                    </div>
                  </div>
                </div>
            </section>
           <p> We evaluate URDFormer on both tabletop manipulation with a UR5 as well as a mobile manipulation task with a Stretch. All videos are played at the speed of x8</p>

          </div>
      </div>

</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">2. Train and Benchmark in Simulation with real-world scenes</h2>
      <h2 class="subtitle">We also created a Gym environment named Reality Gym, featuring assets originated from internet images. </h2>
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/reality_gym.gif">
            </figure>
            <p class="has-text-centered">(a) We define 4 main tasks: (1) Open [any articulated part] (2) Close [any articulated part]
            (3) Fetch the object (4) Collect the object. Successful demonstrations are automatically generated using Curobo, togehter
              with corresponding language descriptions.
            </p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/domain_randomization.gif">
            </figure>
            <p class="has-text-centered">(b) We also provide augmentation for switching (1) meshes for handles, frames and drawer etc, using Partnet dataset (Mo et al., 2019)
            and (2) textures using stable diffusion (Robin Rombach et al., 2022).
            </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Line separator. -->
<div class="columns is-centered">
  <div class="column is-two-thirds">
    <hr class="is-divider" style="height: 4px;">
  </div>
</div>
<!--/ Line separator. -->


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Method Details</h2>
      <h2 class="title is-4">URDFormer: Forward (Generating Photorealistic Image-URDF pairs)</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/inpainting_vs_ours.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
             <p>
              <strong>(a) Motivation:</strong> Given initial poorly-rendered simulation images, we want to convert them into photorealistic images. However, if we directly use off-the-shelf stable diffusion (Robin Rombach et al., 2022) models,
              they usually ignore the local configuration and produce images that do not match the original structures.
             </p>
          </div>
          </div>
          <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/part_aware.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
            <strong>(b) Step-by-step Approach:</strong> Instead, we first collect a small set of texture images and use stable
            diffusion to generate a much larger dataset of texture images. We then apply perspective warping to put a randomly selected texture image on the region of the image and iteratively
            apply this step for all the parts of the object. This simple approach surprisingly works well at preserving local structures while providing diverse realism to the original image.
         </p>
          </div>
          </div>
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/part_consistent_cabs.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
           <strong>(c) Part-aware Generation:</strong> Using this approach, we can now convert original synthetic images such as cabinets into photorealistic images that render the correct kinematic parts and configurations.
         </p>
          </div>
        </div>

        </br>
        </br>

        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/data_generation.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
            <p>
            <strong>(d) Image-URDF Dataset:</strong> Photorealistic training set converted from the original synthetic images can be used for training URDFormer.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Inverse (Internet Images to URDFs)</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/model_soup.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
            <strong> (a) Object and Part Detection: </strong> Our network takes bounding box of objects or parts and predict their kinematic structures.
            The bbox labels can be easily obtained in simulation during forward step, but how about when we apply this to the real-world images?
            Interestingly, the photo-realistic dataset generated during the forward phase (see above (c) and (d)) can also be used to fine-tune an object detection model.
            We fine-tune GroundingDINO (Liu et al., 2023), an open-vocabulary object detection model, on our generated dataset to improve detection of parts such as "oven door," "knob," and "drawer."
            </br>
            <strong> (b) Model Soup Approach for Finetuning: </strong> When evaluating the fine-tuned model, although its performance shows visible improvement, with a 13% gain in F1 score for cabinet images,
            we also observe a decreased ability to detect cases such as "a weird-shaped handle." This is likely because the fine-tuned model starts to
            "give up" some knowledge learned from the pretrained large real-world dataset to fit into the generated dataset. Inspired by insights from
            the recent work on Model Soups (Wortsman et al., 2022), we combine the "knowledge" from both the pretrained model and the fine-tuned model
            by simply averaging their weights. This approach leads to surprising results: The F1 score is further improved by over 10%.
          </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Network</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/urdformer.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
            <p>
            <strong> Network Architecture: </strong> Using the generated dataset, we can now train a transformer-based network called URDFormer. Note that URDFormer takes both RGB images as well as
            bounding boxes of its parts, such as handles and drawers. During training, the bounding boxes are automatically obtained using the simulator.
            During inference time, we fine-tune GroundingDINO using the model soup approach (see more details below). For each image crop, URDFormer
            classifies the mesh type, position (discretized), scale (discretized), and parent. These URDF primitives are fed into a URDF template to
            create the final URDF that can be loaded into a simulator. For global scene prediction, we train two separate networks: URDFormer (Global)
            focuses on predicting the parent and spatial information on how to place the object. URDFormer (Part) takes the cropped image containing
            each object and predicts the detailed structure. The results of the two predictions are combined to create the full scene prediction.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Limitations</h2>
      <h2 class="subtitle">Please see the full list of limitations and future work in our paper </h2>
      <p class="has-text-centered">
        (a) Part Detection: the performance of URDFormer still relies on the performance of bounding box detection.
        (b) Texture and Meshes: URDFormer focuses on predicting kinematic URDF structures and uses predefined meshes that might not match the real-world scenes. To apply textures, we simply assume all parts are rectangular shapes, and use the bounding box of each object part to crop the image and import it into a uv map template. However, this does not work for irregular meshes such as a donut-shape door, or when the object in the image is tilted.
        (c) Primitives URDFormer currently only supports articulated objects that have limited joint types such as prismatic and revolute, and cannot predict complex objects such as cars and lamps.
        (d) Limited URDF primitives: URDFormer does not predict physics parameters such as mass or friction.
      </p>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-fullhd">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">BibTeX</span></h2>
    <pre><code>@article{chen2024urdformer,
  title={URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images},
  author={Zoey Chen and Aaron Walsman and Marius Memmel and Kaichun Mo and Alex Fang and Karthikeya Vemuri and Alan Wu and Dieter Fox and Abhishek Gupta},
  year={2024}
}</code></pre>
  </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/cliport/clipoprt.github.io">CLIPort</a> made by the amazing <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and  <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
