<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images">
  <meta name="keywords" content="URDFormer, real-to-sim, robot manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>URDFormer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4YJFFHEBP9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4YJFFHEBP9');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://qiuyuchen14.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://sites.google.com/view/implicitaugmentation/home">
            ISAGrasp
          </a>
          <a class="navbar-item" target="_blank" href="https://genaug.github.io/">
            URDFormer
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images</h1>
<!--          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">Under Submission</a></h3>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://qiuyuchen14.github.io/">Zoey Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://aaronwalsman.com/">Aaron Walsman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://memmelma.github.io/">Marius Memmel </a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://kaichun-mo.github.io/">Kaichun Mo</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=kD9uKC4AAAAJ&hl=en">Alex Fang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/karthikeya-vemuri/">Karthikeya Vemuri</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/alan-wu-501a93202/">Alan Wu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox*</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://abhishekunique.github.io/">Abhishek Gupta*</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Nvidia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2302.06671"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://urdformer.github.io/media/videos/GenAug_voice.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/urdformer/urdformer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <img src="https://urdformer.github.io/media/images/teaser.gif" class="interpolation-image"
         alt="Interpolate start reference image." />
      <h2 class="subtitle has-text-centered">
      </br>
        We propose <span class="dcliport">URDFormer</span>, a pipeline towards large-scale simulation generation from real-world images.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
<!--        <h2 class="title is-3"><span class="dcliport">GenAug</span></h2>-->

        <!-- Interpolating. -->
        <h3 class="title is-4"> <b>URDFOrmer Application</b></h3>
        <img src="https://urdformer.github.io/media/images/application.png" class="interpolation-image"
         alt="Interpolate start reference image." />
        <br/>
        <br/>
          Examples of application for URDFormer. (a) To safely deploy a robot in the real world, we first generate the "digital" version of the real-world scene, then train a robot in simulation and deploy in the real-world.
        (b) Since URDFormer is based on RGB images, we can potentially generate large-scale simulation environment from internet data that can emulate the real-world distrubtion. 
        <br/>

     </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision.
            Achieving this goal since it provides a realistic, targeted simulation playground for training generalizable decision-making systems.
            This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems.
            However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
            While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes,
            complete with ``natural" kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets.
            To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models.
            We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures
            from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments
            and an integrated system for training robust robotic control policies in the resulting environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
    <!-- Paper video. -->
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MxcmKKvdBhk?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">Articulated Digital Twins from Images</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
      URDFormer enables fast simulation content generation directly from images, either from internet or captured from your phone.
    </h2>
    <div class="container">
      <img src="https://urdformer.github.io/media/images/urdformer_cabinets.gif" class="interpolation-image"
         alt="Interpolate start reference image." />
      <h2 class="subtitle has-text-centered">
      </br>
       Predicted URDFs of cabinets with different configurations from internet images
      </h2>

      <img src="https://urdformer.github.io/media/images/urdformer_appliance.gif" class="interpolation-image"
         alt="Interpolate start reference image." />
      <h2 class="subtitle has-text-centered">
      </br>
        Predicted URDFs of different kitchen appliances from interenet images.
      </h2>

      <img src="https://urdformer.github.io/media/images/urdformer_phone_kitchens.gif" class="interpolation-image"
         alt="Interpolate start reference image." />
      <h2 class="subtitle has-text-centered">
      </br>
        We also visualize several examples of predicted URDFs of real-world kitchens captured by a phone.
      </h2>

    </div>
  </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">Real-World Experiments</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
       By training on a dataset extrapolated from only 10 demonstrations in this simple environment on the left,
      the robot is able
        </br>
        to solve the task in entirely different environments and objects.
    </h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/real_world1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/real_world1.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop height="100%">
            <source src="https://urdformer.github.io/media/videos/real_world2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">Reality Gym</span></h2>
      <h2 class="subtitle has-text-centered">
    </br>
       URDFormer enables efficient interactive scene generation directly from images, this allows us to create environments that emulate the real-world distribution.
        We introduce Reality Gym, an Gym environment that imports assets generated from internet images and automatically create language conditioned demonstrations, for tasks on
        "open [any articulated parts]", "close [any articulated parts]", "Fetch [the object]" and "collect [the object]". We leverage the privilate information obtained from simulation,
        to automatically fill in the part names from their spatial information, such as "top middle drawer" or "bottom left door". We combine motion planning from Curobo and create
        a successful dataset to train a robot and complate these tasks on the internet-originated assets.
    </h2>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-fullbody">
          <img src="https://urdformer.github.io/media/images/reality_gym.gif" class="interpolation-image"
         alt="Interpolate start reference image." />
        </div>

      </div>
    </div>
  </div>
  </div>
</section>









<section class="section" id="BibTeX">
  <div class="container is-fullhd">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">BibTeX</span></h2>
    <pre><code>@article{chen2023genaug,
  title={GenAug: Retargeting behaviors to unseen situations via Generative Augmentation},
  author={Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  journal={arXiv preprint arXiv:2302.06671},
  year={2023}
}</code></pre>
  </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/cliport/clipoprt.github.io">CLIPort</a> made by the amazing <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and  <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
