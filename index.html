<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images">
  <meta name="keywords" content="URDFormer, real-to-sim, robot manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>URDFormer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4YJFFHEBP9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4YJFFHEBP9');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://qiuyuchen14.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://sites.google.com/view/implicitaugmentation/home">
            ISAGrasp
          </a>
          <a class="navbar-item" target="_blank" href="https://genaug.github.io/">
            GenAug
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">URDFormer: A Pipeline for Constructing Articulated Simulation Environments from Real-World Images</h1>
<!--          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">Under Submission</a></h3>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://qiuyuchen14.github.io/">Zoey Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://aaronwalsman.com/">Aaron Walsman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://memmelma.github.io/">Marius Memmel </a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://kaichun-mo.github.io/">Kaichun Mo</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://scholar.google.com/citations?user=kD9uKC4AAAAJ&hl=en">Alex Fang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/karthikeya-vemuri/">Karthikeya Vemuri</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/alan-wu-501a93202/">Alan Wu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox*</a><sup>1,2</sup>
            </span>
            <span class="author-block">
              <a target="_blank" href="https://abhishekunique.github.io/">Abhishek Gupta*</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Nvidia, </span>
            <span class="author-block"><sup>*</sup>Equal Advising</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://drive.google.com/file/d/1vXcBgf--ySQWeh3VFIiigAV8_cZi_Kzv/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a target="_blank" href="https://urdformer.github.io/media/videos/urdformer_final.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://github.com/urdformer/urdformer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
               <!-- dataset Link. -->
<!--              <span class="link-block">-->
<!--                <a target="_blank" href="https://github.com/urdformer/reality_gym"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Reality Gym (Coming soon!)</span>-->
<!--                  </a>-->
<!--              </span>-->
<!--              <span class="link-block">-->
<!--                <a target="_blank" href="https://github.com/urdformer/robot_setup"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Real-World Setup (Coming soon!)</span>-->
<!--                  </a>-->
<!--              </span>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/teaser_final.gif">
            </figure>
            <p class="has-text-centered">
            We propose URDFormer, a scalable pipeline towards large-scale simulation generation from real-world images.
              Given an image (from the internet or captured from a phone), URDFormer predicts its corresponding interactive "digital twin"
              in the format of a URDF. This URDF can be loaded into a simulator to train a robot for different tasks.
            </p>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer Applications</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/URDFormer_application.gif" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
           (a) For the safe deployment of robots in real-world settings, we initially create a 'digital' replica of the real-world scene.
            Subsequently, we gather demonstrations in simulation and train a robot policy. This policy can then be safely deployed in the real world or used for further real-world fine-tuning.
            (b) Given that URDFormer relies solely on RGB images, it holds the potential to create extensive simulation environments from internet data, closely mirroring the real-world distribution.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Constructing accurate and targeted simulation scenes that are both visually and physically realistic is a problem of significant practical interest in domains ranging from robotics to computer vision.
            Achieving this goal since it provides a realistic, targeted simulation playground for training generalizable decision-making systems.
            This problem has become even more relevant as researchers wielding large data-hungry learning methods seek new sources of training data for physical decision-making systems.
            However, building simulation models is often still done by hand - a graphic designer and a simulation engineer work with predefined assets to construct rich scenes with realistic dynamic and kinematic properties.
            While this may scale to small numbers of scenes, to achieve the generalization properties that are required for data-driven robotic control, we require a pipeline that is able to synthesize large numbers of realistic scenes,
            complete with ``natural" kinematic and dynamic structure. To attack this problem, we develop models for inferring structure and generating simulation scenes from natural images, allowing for scalable scene generation from web-scale datasets.
            To train these image-to-simulation models, we show how controllable text-to-image generative models can be used in generating paired training data that allows for modeling of the inverse problem, mapping from realistic images back to complete scene models.
            We show how this paradigm allows us to build large datasets of scenes in simulation with semantic and physical realism. We present an integrated end-to-end pipeline that generates simulation scenes complete with articulated kinematic and dynamic structures
            from real-world images and use these for training robotic control policies. We then robustly deploy in the real world for tasks like articulated object manipulation. In doing so, our work provides both a pipeline for large-scale generation of simulation environments
            and an integrated system for training robust robotic control policies in the resulting environments.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
    <!-- Paper video. -->
    </br>
    </br>
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://urdformer.github.io/media/videos/urdformer_final.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

</section>

<!-- Line separator. -->
<div class="columns is-centered">
  <div class="column is-two-thirds">
    <hr class="is-divider" style="height: 4px;">
  </div>
</div>
<!--/ Line separator. -->

<!--<section class="hero teaser">-->
<!--  <div class="columns is-centered has-text-centered">-->
<!--    <div class="hero-body">-->
<!--    <h2 class="title is-3">URDFormer: Overview</h2>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--    </br>-->
<!--    </h2>-->
<!--    <div class="container">-->
<!--      <img src="https://urdformer.github.io/media/images/pipeline.png" class="interpolation-image"-->
<!--         alt="Interpolate start reference image." />-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--      </br>-->
<!--        To train a network that takes real-world images and predicts their corresponding URDFs, we propose a forward-inverse pipeline. During Forward phase,-->
<!--        we first randomly sample a URDF (This could be from a dataset or procedually generated) and load corresponding articulated assets. However, if we train a network-->
<!--        with the initial texture, it likely won't generalize well to real-world images due to distribution shift. We leverage the power of image-conditioned stable diffusion-->
<!--        that was pretrained on web-scale dataset, and convert the initial synthetic images into photo-realistic images. Now we automatically have paired realistic images with-->
<!--        their corresponding URDFs to supervise a network. Our hope is that the generated images can emulated the real-world complexicity such as lightings and textures, and-->
<!--        enable the network predict correct URDFs given real-world images during the inverse phase.-->
<!--      </h2>-->

<!--    </div>-->
<!--  </div>-->
<!--  </div>-->
<!--</section>-->
<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Overview</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/pipeline.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
            To train a network that takes real-world images and predicts their corresponding URDFs, we propose a forward-inverse pipeline. During the forward phase,
            we first randomly sample a URDF (this could be from a dataset or procedurally generated) and load the corresponding articulated assets.
            However, if we train a network with the initial texture, it likely won't generalize well to real-world images due to the distribution shift.
            We leverage the power of image-conditioned stable diffusion (Robin Rombach et al., 2022) that was pretrained on a web-scale dataset and convert the initial synthetic images into photorealistic images.
            Now we automatically have paired realistic images with their corresponding URDFs to supervise a network. Our hope is that the generated images can emulate real-world complexity, such as lighting and textures,
            and enable the network to predict correct URDFs given real-world images during the inverse phase.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Forward (Data Generation)</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-one-third">
            <img src="https://urdformer.github.io/media/images/inpainting_vs_ours.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          </div>
          <div class="column is-one-third">
            <img src="https://urdformer.github.io/media/images/step_by_step.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/part_consistent_cabs.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
          Given initial poorly-rendered simulation images, we want to convert them into photorealistic images. However, if we directly use off-the-shelf stable diffusion (Robin Rombach et al., 2022) models,
          they usually ignore the local configuration and produce images that do not match the original structures. Instead, we first collect a small set of texture images and use stable
          diffusion to generate a much larger dataset of texture images. We then apply perspective warping to put a randomly selected texture image on the region of the image and iteratively
          apply this step for all the parts of the object. This simple approach surprisingly works well at preserving local structures while providing realism to the original image.
         </p>
          </div>
        </div>

        </br>
        </br>

        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/data_generation.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
            <p>
            Examples of generated training set given the original synthetic images.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Forward (Network)</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/urdformer.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
            <p>
            Using the generated dataset, we can now train a transformer-based network called URDFormer. Note that URDFormer takes both RGB images as well as
            bounding boxes of its parts, such as handles and drawers. During training, the bounding boxes are automatically obtained using the simulator.
            During inference time, we fine-tune GroundingDINO using the model soup approach (see more details below). For each image crop, URDFormer
            classifies the mesh type, position (discretized), scale (discretized), and parent. These URDF primitives are fed into a URDF template to
            create the final URDF that can be loaded into a simulator. For global scene prediction, we train two separate networks: URDFormer (Global)
            focuses on predicting the parent and spatial information on how to place the object. URDFormer (Part) takes the cropped image containing
            each object and predicts the detailed structure. The results of the two predictions are combined to create the full scene prediction.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-4">URDFormer: Inverse (Detection)</h2>
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-two-thirds">
            <img src="https://urdformer.github.io/media/images/model_soup.png" class="interpolation-image" alt="Interpolate start reference image." style="max-width: 100%; height: auto;" />
          <p>
            Interestingly, the photo-realistic dataset generated during the forward phase can also be used to fine-tune an object detection model. We fine-tune GroundingDINO (Liu et al., 2023),
            an open-vocabulary object detection model, on our generated dataset to improve detection of parts such as "oven door," "knob," and "drawer."
            When evaluating the fine-tuned model, although its performance shows visible improvement, with a 13% gain in F1 score for cabinet images,
            we also observe a decreased ability to detect cases such as "a weird-shaped handle." This is likely because the fine-tuned model starts to
            "give up" some knowledge learned from the pretrained large real-world dataset to fit into the generated dataset. Inspired by insights from
            the recent work on Model Soups (Wortsman et al., 2022), we combine the "knowledge" from both the pretrained model and the fine-tuned model
            by simply averaging their weights. This approach leads to surprising results: The F1 score is further improved by over 10%.
          </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- Line separator. -->
<div class="columns is-centered">
  <div class="column is-two-thirds">
    <hr class="is-divider" style="height: 4px;">
  </div>
</div>
<!--/ Line separator. -->


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Experiments: Articulated Digital Twins</h2>
      <h2 class="subtitle">URDFormer enables fast simulation content generation directly from images, whether they're sourced from the internet or captured with your phone.</h2>
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_cabinets.gif" alt="Predicted URDFs of cabinets with different configurations from internet images">
            </figure>
            <p class="has-text-centered">(a) Predicted URDFs for cabinets in various configurations derived from internet images.</p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_appliance.gif" alt="Predicted URDFs of different kitchen appliances from internet images">
            </figure>
            <p class="has-text-centered">(b) Predicted URDFs for kitchen appliances in various configurations derived from internet images.</p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/urdformer_phone_kitchens.gif" alt="Predicted URDFs of real-world kitchens captured by a phone">
            </figure>
            <p class="has-text-centered">(c) We also visualize several examples of predicted URDFs from real-world kitchens captured using a phone.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Experiments: Real-to-Sim-to-Real for safer real-world deployment</h2>
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/real_world1.gif">
            </figure>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/real_world2.gif">
            </figure>
             <p>Given a single image of an object, URDFormer predicts its URDF, automatically collects data,
               and trains a robot within a simulator. The learned policy can then be transferred to the real world for safer deployment.</p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
    <div class="hero-body">
      <h2 class="title is-3">Experiments: Reality Gym</h2>
      <h2 class="subtitle">We have created a Gym environment named Reality Gym, featuring assets originated from internet images. We will release Reality Gym soon and stay tuned!</h2>
      <div class="container">
        <div class="columns is-multiline is-centered">
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/reality_gym.gif">
            </figure>
            <p class="has-text-centered">(a) We define 4 main tasks: (1) Open [any articulated part] (2) Close [any articulated part]
            (3) Fetch the object (4) Collect the object. Successful demonstrations are automatically generated using Curobo, togehter
              with corresponding language descriptions.
            </p>
          </div>
          <div class="column is-two-thirds">
            <figure class="image">
              <img src="https://urdformer.github.io/media/images/domain_randomization.gif">
            </figure>
            <p class="has-text-centered">(b) We also provide augmentation for switching (1) meshes for handles, frames and drawer etc, using Partnet dataset (Mo et al., 2019)
            and (2) textures using stable diffusion (Robin Rombach et al., 2022).
            </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>
<section class="section" id="BibTeX">
  <div class="container is-fullhd">
    <div class="hero-body">
    <h2 class="title is-3"><span class="dcliport">BibTeX</span></h2>
    <pre><code>@article{chen2024urdformer,
  title={URDFormer},
  author={},
  journal={TBD},
  year={2024}
}</code></pre>
  </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/cliport/clipoprt.github.io">CLIPort</a> made by the amazing <a href="https://mohitshridhar.com/">Mohit Shridhar</a> and  <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
